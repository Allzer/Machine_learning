{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allzer/Machine_learning/blob/main/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbq75QumGuuy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import scipy.io as sio\n",
        "from sklearn.model_selection import train_test_split #разбивает данные для тестирования и тренировки\n",
        "\n",
        "#библиотеки курса\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#загружаем тестовые данные\n",
        "def load_data_mat(filename, max_samples, seed=64):\n",
        "    raw = sio.loadmat(filename) #загружаем файл\n",
        "    x = raw['X'] #Передаём массивы принадлежащие Х\n",
        "    y = raw['y'] #Передаём массивы принадлежащие у\n",
        "\n",
        "    x = np.moveaxis(x, [3], [0]) #Перемещение оси массива. 3 ось мы ставим на нулевую\n",
        "    y = y.flatten() #Возвращает одномерный массив (для уменьшения массива до одного измерения)\n",
        "\n",
        "    y[y == 10] = 0 #все значения 10 присваивается знаение 0\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    samples = np.random.choice(np.arange(x.shape[0]), max_samples, replace=False) #x.shape[0] передаёт в данном случае\n",
        "    #рандомную строку 2D массива\n",
        "    #np.choice - вернет max_sample случайных чисeл от 0 до x.shape[0].\n",
        "    return x[samples].astype(np.float32), y[samples] #возвращаем массив иксов и игриков с одинаковыми семплами\n",
        "\n",
        "def load_svhn(folder, max_train, max_test):\n",
        "\n",
        "    #train_x - массив (num_train,32,32,3) - тренировочные изображения\n",
        "    #train_y - массив (num_train) - training labels - тренировочный разделитель\n",
        "    #test_X, np array (num_test, 32, 32, 3) - тестовые изображения\n",
        "    #test_y, np array of int (num_test) - тестовый разделитель\n",
        "\n",
        "    train_X, train_y = load_data_mat(os.path.join(folder, \"train_32x32.mat\"), max_train) #Загружает данные для тренировки\n",
        "    test_X, test_y = load_data_mat(os.path.join(folder, \"test_32x32.mat\"), max_test) #Загружает данные для тестов\n",
        "    return train_X, train_y, test_X, test_y #Возвращает загруенные данные\n",
        "\n",
        "def random_split_train_val(X, y, num_val, seed=35):\n",
        "    '''\n",
        "    Randomly splits dataset into training and validation\n",
        "\n",
        "    Arguments:\n",
        "    X - np-массив с выборками\n",
        "    y - np-массив с метками\n",
        "    num_val - количество образцов для проверки\n",
        "    seed - random seed\n",
        "\n",
        "    Returns:\n",
        "    train_X, np array (num_train, 32, 32, 3) - обучающие изображения\n",
        "    train_y, np array of int (num_train) - обучающие метки\n",
        "    val_X, np array (num_val, 32, 32, 3) - validation images\n",
        "    val_y, np array of int (num_val) - validation labels\n",
        "    '''\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    indices = np.arange(X.shape[0]) #передаёт в переменную одномерны массив\n",
        "    np.random.shuffle(indices) #измените последовательность на месте, перетасовав ее содержимое.\n",
        "\n",
        "    train_indices = indices[:-num_val] #получаем все элементы массива кроме элемента с индексом -num_val\n",
        "    train_X = X[train_indices]\n",
        "    train_y = y[train_indices]\n",
        "\n",
        "    val_indices = indices[-num_val:]\n",
        "    val_X = X[val_indices]\n",
        "    val_y = y[val_indices]\n",
        "\n",
        "    return train_X, train_y, val_X, val_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhvU6PJQFn6y"
      },
      "outputs": [],
      "source": [
        "def prepare_for_linear_classifier(train_X, test_X):\n",
        "    train_flat = train_X.reshape(train_X.shape[0], -1) / 255\n",
        "    test_flat = test_X.reshape(test_X.shape[0], -1) / 255\n",
        "\n",
        "    # Subtract mean\n",
        "    mean_image = np.mean(train_flat, axis = 0)\n",
        "    train_flat -= mean_image\n",
        "    test_flat -= mean_image\n",
        "\n",
        "    # Add another channel with ones as a bias term\n",
        "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
        "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])\n",
        "    return train_flat_with_ones, test_flat_with_ones\n",
        "\n",
        "train_X, train_y, test_X, test_y = load_svhn(\"drive/MyDrive/data\", max_train=10000, max_test=1000)\n",
        "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
        "# Split train into train and val\n",
        "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dkIKLn7IaOF"
      },
      "source": [
        "Играемся с градиентами!\n",
        "\n",
        "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.\n",
        "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZv0wouGKApj"
      },
      "outputs": [],
      "source": [
        "CRED = '\\033[91m'\n",
        "CGREN = '\\033[42m'\n",
        "CEND = '\\033[0m'\n",
        "def check_gradient(f, x, delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks the implementation of analytical gradient by comparing\n",
        "    it to numerical gradient using two-point formula\n",
        "\n",
        "    Arguments:\n",
        "      f: function that receives x and computes value and gradient\n",
        "      x: np array, initial point where gradient is checked\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "\n",
        "    Return:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    assert isinstance(x, np.ndarray)\n",
        "    assert x.dtype == np.float\n",
        "\n",
        "    fx, analytic_grad = f(x)\n",
        "    analytic_grad = analytic_grad.copy()\n",
        "\n",
        "\n",
        "    assert analytic_grad.shape == x.shape, (analytic_grad.shape, x.shape)\n",
        "\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "        analytic_grad_at_ix = analytic_grad[ix]\n",
        "        numeric_grad_at_ix = 0\n",
        "\n",
        "        # TODO Copy from previous assignment\n",
        "        x_plus_h = x.copy()\n",
        "        x_minus_h = x.copy()\n",
        "\n",
        "        x_plus_h[ix] += delta\n",
        "        x_minus_h[ix] -= delta\n",
        "        #print(\"X -\", x_minus_h)\n",
        "        #print(\"X +\", x_plus_h)\n",
        "        numeric_grad_at_ix = (f(x_plus_h)[0] - f(x_minus_h)[0])/(2.*delta)\n",
        "\n",
        "        if not np.isclose(numeric_grad_at_ix, analytic_grad_at_ix, tol):\n",
        "            print(CRED + \"Gradients are different at %s. Analytic: %2.5f, Numeric: %2.5f\" % (ix, analytic_grad_at_ix, numeric_grad_at_ix) + CEND)\n",
        "            return False\n",
        "\n",
        "        it.iternext()\n",
        "\n",
        "    print(CGREN+ \"Gradient check passed!\" + CEND)\n",
        "    return True, numeric_grad_at_ix\n",
        "\n",
        "\n",
        "def check_layer_gradient(layer, x, delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks gradient correctness for the input and output of a layer\n",
        "\n",
        "    Arguments:\n",
        "      layer: neural network layer, with forward and backward functions\n",
        "      x: starting point for layer input\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "\n",
        "    Returns:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    output = layer.forward(x)\n",
        "    #print(\"OUTPUT\", output)\n",
        "    output_weight = np.random.randn(*output.shape)\n",
        "\n",
        "    def helper_func(x):\n",
        "        output = layer.forward(x)\n",
        "        loss = np.sum(output * output_weight)\n",
        "        d_out = np.ones_like(output) * output_weight\n",
        "        grad = layer.backward(d_out)\n",
        "        return loss, grad\n",
        "\n",
        "    return check_gradient(helper_func, x, delta, tol)\n",
        "\n",
        "\n",
        "def check_layer_param_gradient(layer, x, param_name, delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks gradient correctness for the parameter of the layer\n",
        "\n",
        "    Arguments:\n",
        "      layer: neural network layer, with forward and backward functions\n",
        "      x: starting point for layer input\n",
        "      param_name: name of the parameter\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "\n",
        "    Returns:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    param = layer.params()[param_name]\n",
        "    initial_w = param.value\n",
        "\n",
        "    output = layer.forward(x)\n",
        "    output_weight = np.random.randn(*output.shape)\n",
        "\n",
        "    def helper_func(w):\n",
        "        param.value = w\n",
        "        output = layer.forward(x)\n",
        "        loss = np.sum(output * output_weight)\n",
        "        d_out = np.ones_like(output) * output_weight\n",
        "        layer.backward(d_out)\n",
        "        grad = param.grad\n",
        "        return loss, grad\n",
        "\n",
        "    return check_gradient(helper_func, initial_w, delta, tol)\n",
        "\n",
        "\n",
        "def check_model_gradient(model, X, y, delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks gradient correctness for all model parameters\n",
        "\n",
        "    Arguments:\n",
        "      model: neural network model with compute_loss_and_gradients\n",
        "      X: batch of input data\n",
        "      y: batch of labels\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "\n",
        "    Returns:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    params = model.params()\n",
        "\n",
        "    for param_key in params:\n",
        "        print(\"Checking gradient for %s\" % param_key)\n",
        "        param = params[param_key]\n",
        "        initial_w = param.value\n",
        "\n",
        "        def helper_func(w):\n",
        "            param.value = w\n",
        "            loss = model.compute_loss_and_gradients(X, y)\n",
        "            grad = param.grad\n",
        "            return loss, grad\n",
        "\n",
        "        if not check_gradient(helper_func, initial_w, delta, tol):\n",
        "            return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOxGRhATHAvn"
      },
      "outputs": [],
      "source": [
        "def square(x):\n",
        "    return x*x, 2*x\n",
        "\n",
        "check_gradient(square, np.array([3.0]))\n",
        "\n",
        "def array_sum(x):\n",
        "    assert x.shape == (2,), x.shape\n",
        "    return np.sum(x), np.ones_like(x)\n",
        "\n",
        "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
        "\n",
        "def array_2d_sum(x):\n",
        "    assert x.shape == (2,2)\n",
        "    return np.sum(x), np.ones_like(x)\n",
        "\n",
        "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLsmr9aVL5Gn"
      },
      "source": [
        "Начинаем писать свои функции, считающие аналитический градиент\n",
        "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
        "\n",
        "Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
        "\n",
        "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
        "\n",
        "predictions -= np.max(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-b-2JnRL40R",
        "outputId": "84ce6133-20ea-4c52-c715-66afac340c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.38389653e-87 3.72007598e-44 1.00000000e+00]\n",
            "[1. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# TODO Implement softmax and cross-entropy for single sample\n",
        "probs = linear_classifer.softmax(np.array([-100, 0, 100]))\n",
        "print(probs)\n",
        "\n",
        "# Убедитесь, что это работает и для больших чисел тоже!\n",
        "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
        "print(probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEUpDEAkz_dl",
        "outputId": "52e70163-7c41-4a5f-a117-78f4f9e56b2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.006760443547122"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.seterr(divide = 'ignore')\n",
        "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
        "linear_classifer.cross_entropy_loss(probs,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vcMOglS2dF8",
        "outputId": "4010ac9a-7007-421b-c682-1c3f97299b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[42mGradient check passed!\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-42f49b5c0f51>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([1])), np.array([1, 0, 0], np.float))\n",
            "<ipython-input-3-f218ed4700ca>:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  assert x.dtype == np.float\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(True, 0.2119415576151695)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([[1, 0, 0]]), np.array([1]))\n",
        "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([1])), np.array([1, 0, 0], np.float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ZpdETlYf6h"
      },
      "source": [
        "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов.\n",
        "\n",
        "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из num_classes оценок, а матрица размерности batch_size, num_classes. Индекс примера в батче всегда будет первым измерением.\n",
        "\n",
        "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
        "\n",
        "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6aooY4VUL65",
        "outputId": "c1724d98-f53e-44c9-faf8-02eb92f9aafe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[42mGradient check passed!\u001b[0m\n",
            "\u001b[42mGradient check passed!\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-725c44ab9862>:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
            "<ipython-input-11-725c44ab9862>:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
            "<ipython-input-3-f218ed4700ca>:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  assert x.dtype == np.float\n",
            "<ipython-input-11-725c44ab9862>:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
            "<ipython-input-11-725c44ab9862>:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "# Test batch_size = 1\n",
        "num_classes = 4\n",
        "batch_size = 1\n",
        "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
        "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
        "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
        "\n",
        "# Test batch_size = 3\n",
        "num_classes = 4\n",
        "batch_size = 3\n",
        "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
        "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
        "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
        "\n",
        "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
        "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBC7xx7ZYoiL"
      },
      "source": [
        "Наконец, реализуем сам линейный классификатор!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-ZtnRROYb3p"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "num_classes = 2\n",
        "num_features = 3\n",
        "np.random.seed(42)\n",
        "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
        "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
        "target_index = np.ones(batch_size, dtype=np.int)\n",
        "\n",
        "loss, dW = linear_classifer.linear_softmax(X, W,target_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE4PP0LrohoO"
      },
      "source": [
        "И теперь регуляризация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYgw77CXnTN-"
      },
      "outputs": [],
      "source": [
        "linear_classifer.l2_regularization(W, 0.01)\n",
        "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItImhJ1DojCJ"
      },
      "source": [
        "Тренировка!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny3AUFPzx6P2"
      },
      "outputs": [],
      "source": [
        "class linear_classifer:\n",
        "  def softmax(predictions):\n",
        "    #Принимаем вектор predictions (получаем его после умножения вектора X @ W)\n",
        "    pred = predictions.copy()\n",
        "    pred = pred - np.max(pred)\n",
        "    Smax = np.zeros(pred.shape)\n",
        "\n",
        "    it = np.nditer(pred,flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "      ix = it.multi_index\n",
        "      Smax[ix] = np.exp(pred[ix])/np.sum(np.exp(pred))\n",
        "      it.iternext()\n",
        "    return Smax\n",
        "    raise Exception('Not implemented!')\n",
        "\n",
        "\n",
        "\n",
        "  def cross_entropy_loss(probs, target_index):\n",
        "      '''\n",
        "      Computes cross-entropy loss\n",
        "\n",
        "      Arguments:\n",
        "        probs, np array, shape is either (N) or (batch_size, N) -\n",
        "          probabilities for every class\n",
        "        target_index: np array of int, shape is (1) or (batch_size) -\n",
        "          index of the true class for given sample(s)\n",
        "\n",
        "      Returns:\n",
        "        loss: single value\n",
        "      '''\n",
        "      # TODO implement cross-entropy\n",
        "      # Your final implementation shouldn't have any loops\n",
        "      return(-np.log(probs[target_index]))\n",
        "      raise Exception(\"Not implemented!\")\n",
        "\n",
        "\n",
        "  def softmax_with_cross_entropy(predictions, target_index):\n",
        "      '''\n",
        "      Computes softmax and cross-entropy loss for model predictions,\n",
        "      including the gradient\n",
        "\n",
        "      Arguments:\n",
        "        predictions, np array, shape is either (N) or (batch_size, N) -\n",
        "          classifier output\n",
        "        target_index: np array of int, shape is (1) or (batch_size) -\n",
        "          index of the true class for given sample(s)\n",
        "\n",
        "      Returns:\n",
        "        loss, single value - cross-entropy loss\n",
        "        dprediction, np array same shape as predictions - gradient of predictions by loss value\n",
        "      '''\n",
        "      # TODO implement softmax with cross-entropy\n",
        "      # Your final implementation shouldn't have any loops\n",
        "\n",
        "      if predictions.ndim == 1:\n",
        "        predictions_new = predictions - np.max(predictions)\n",
        "      else:\n",
        "          maximum = np.max(predictions, axis=1)\n",
        "          predictions_new = predictions - maximum[:, np.newaxis]\n",
        "      predictions_new = np.exp(predictions_new)\n",
        "      predictions_sum = np.sum(predictions_new, axis=(predictions.ndim - 1))\n",
        "      if predictions.ndim == 1:\n",
        "          probabilities = predictions_new / predictions_sum\n",
        "      else:\n",
        "          probabilities = predictions_new / predictions_sum[:, np.newaxis]\n",
        "\n",
        "      mask_target = np.zeros(probabilities.shape)\n",
        "      if probabilities.ndim == 1:\n",
        "          mask_target[target_index] = 1\n",
        "      elif target_index.ndim == 1:\n",
        "          mask_target[tuple(np.arange(0, probabilities.shape[0])), tuple(target_index)] = 1\n",
        "      else:\n",
        "          mask_target[tuple(np.arange(0, probabilities.shape[0])), tuple(target_index.T[0])] = 1\n",
        "\n",
        "      loss = -np.sum(mask_target * np.log(probabilities))\n",
        "\n",
        "      dprediction = probabilities\n",
        "      dprediction[mask_target.astype(bool)] = dprediction[mask_target.astype(bool)] - 1\n",
        "\n",
        "      return loss, dprediction\n",
        "\n",
        "\n",
        "  def l2_regularization(W, reg_strength):\n",
        "    '''\n",
        "    Computes L2 regularization loss on weights and its gradient\n",
        "\n",
        "    Arguments:\n",
        "      W, np array - weights\n",
        "      reg_strength - float value\n",
        "\n",
        "    Returns:\n",
        "      loss, single value - l2 regularization loss\n",
        "      gradient, np.array same shape as W - gradient of weight by l2 loss\n",
        "    '''\n",
        "\n",
        "    loss = reg_strength * np.trace(np.matmul(W.T, W))   # L2(W) = λ * tr(W.T * W)\n",
        "    grad = 2 * reg_strength * W                         # dL2(W)/dW = 2 * λ * W\n",
        "\n",
        "    return loss, grad\n",
        "\n",
        "\n",
        "  def linear_softmax(X, W, target_index):\n",
        "      '''\n",
        "      Performs linear classification and returns loss and gradient over W\n",
        "\n",
        "      Arguments:\n",
        "        X, np array, shape (num_batch, num_features) - batch of images\n",
        "        W, np array, shape (num_features, classes) - weights\n",
        "        target_index, np array, shape (num_batch) - index of target classes\n",
        "\n",
        "      Returns:\n",
        "        loss, single value - cross-entropy loss\n",
        "        gradient, np.array same shape as W - gradient of weight by loss\n",
        "\n",
        "      '''\n",
        "      predictions = np.dot(X, W)\n",
        "      loss, dprediction = linear_classifer.softmax_with_cross_entropy(predictions, target_index)\n",
        "      dW = np.dot(X.T, dprediction)\n",
        "\n",
        "      # TODO implement prediction and gradient over W\n",
        "      # Your final implementation shouldn't have any loops\n",
        "      return loss, dW\n",
        "      raise Exception(\"Not implemented!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGNG1CNIqEzU"
      },
      "outputs": [],
      "source": [
        "class LinearSoftmaxClassifier():\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def fit(self, X, y, batch_size=100, learning_rate=1e-7, reg=1e-5, epochs=2):\n",
        "        '''\n",
        "        Trains linear classifier\n",
        "\n",
        "        Arguments:\n",
        "          X, np array (num_samples, num_features) - training data\n",
        "          y, np array of int (num_samples) - labels\n",
        "          batch_size, int - batch size to use\n",
        "          learning_rate, float - learning rate for gradient descent\n",
        "          reg, float - L2 regularization strength\n",
        "          epochs, int - number of epochs\n",
        "        '''\n",
        "\n",
        "        num_train = X.shape[0]\n",
        "        num_features = X.shape[1]\n",
        "        num_classes = np.max(y)+1\n",
        "        if self.W is None:\n",
        "            self.W = 0.001 * np.random.randn(num_features, num_classes)\n",
        "\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            shuffled_indices = np.arange(num_train)\n",
        "            np.random.shuffle(shuffled_indices)\n",
        "            sections = np.arange(batch_size, num_train, batch_size)\n",
        "            batches_indices = np.array_split(shuffled_indices, sections)\n",
        "\n",
        "            # TODO implement generating batches from indices\n",
        "            # Compute loss and gradients\n",
        "            # Apply gradient to weights using learning rate\n",
        "            # Don't forget to add both cross-entropy loss\n",
        "            # and regularization!\n",
        "\n",
        "            batch = X[batches_indices[0]]\n",
        "            target_index = y[batches_indices[0]]\n",
        "            loss_cross, dW = linear_classifer.linear_softmax(batch, self.W, target_index)\n",
        "            loss_reg, grad_reg = linear_classifer.l2_regularization(self.W, reg)\n",
        "            self.W = self.W - learning_rate*(dW+grad_reg)\n",
        "            loss = loss_reg + loss_cross\n",
        "            loss_history.append(loss)\n",
        "            print(\"Epoch %i, loss: %f\" % (epoch, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Produces classifier predictions on the set\n",
        "\n",
        "        Arguments:\n",
        "          X, np array (test_samples, num_features)\n",
        "\n",
        "        Returns:\n",
        "          y_pred, np.array of int (test_samples)\n",
        "        '''\n",
        "        predictions = np.dot(X, self.W)\n",
        "        probabilities = linear_classifer.softmax(predictions)\n",
        "        y_pred = np.argmax(probabilities, axis=1)\n",
        "        return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhMmvizioikv",
        "outputId": "192d955d-1eee-44d0-d85b-95e66b594524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss: 690.986289\n",
            "Epoch 1, loss: 691.113189\n",
            "Epoch 2, loss: 722.857888\n",
            "Epoch 3, loss: 727.012217\n",
            "Epoch 4, loss: 757.146392\n",
            "Epoch 5, loss: 783.371317\n",
            "Epoch 6, loss: 880.354906\n",
            "Epoch 7, loss: 895.022272\n",
            "Epoch 8, loss: 1101.244859\n",
            "Epoch 9, loss: 887.108549\n"
          ]
        }
      ],
      "source": [
        "classifier = LinearSoftmaxClassifier()\n",
        "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "X9gjdaGOzBur",
        "outputId": "b4d4e9ab-40de-4076-c19b-d224bc27eb7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7a922d716110>]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1J59YMPzxR8"
      },
      "outputs": [],
      "source": [
        "def binary_classification_metrics(prediction, ground_truth):\n",
        "    '''\n",
        "    Computes metrics for binary classification\n",
        "\n",
        "    Arguments:\n",
        "    prediction, np array of bool (num_samples) - model predictions\n",
        "    ground_truth, np array of bool (num_samples) - true labels\n",
        "\n",
        "    Returns:\n",
        "    precision, recall, f1, accuracy - classification metrics\n",
        "    '''\n",
        "    tp = np.sum(np.logical_and(prediction, ground_truth))\n",
        "    fp = np.sum(np.greater(prediction, ground_truth))\n",
        "    fn = np.sum(np.less(prediction, ground_truth))\n",
        "    precision = tp/(tp + fp)\n",
        "    recall = tp/(tp+fn)\n",
        "\n",
        "    accuracy = np.sum(prediction==ground_truth)/prediction.size\n",
        "    f1 = precision*recall/(precision+recall)\n",
        "\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "\n",
        "def multiclass_accuracy(prediction, ground_truth):\n",
        "    '''\n",
        "    Computes metrics for multiclass classification\n",
        "\n",
        "    Arguments:\n",
        "    prediction, np array of int (num_samples) - model predictions\n",
        "    ground_truth, np array of int (num_samples) - true labels\n",
        "\n",
        "    Returns:\n",
        "    accuracy - ratio of accurate predictions to total samples\n",
        "    '''\n",
        "    # TODO: Implement computing accuracy\n",
        "\n",
        "    return accuracy_score(ground_truth, prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv0vDwUIzGBN",
        "outputId": "0d0fc35c-bc6b-4820-9dda-d1a8fa6c6dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.161\n"
          ]
        }
      ],
      "source": [
        "pred = classifier.predict(val_X)\n",
        "accuracy = multiclass_accuracy(pred, val_y)\n",
        "print(\"Accuracy: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSuCZeRaz6QO",
        "outputId": "439ac7d5-7d59-4c4a-cdda-7634474b8e18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Accuracy after training for 100 epochs:  0.055\n"
          ]
        }
      ],
      "source": [
        "classifier.fit(train_X, train_y, epochs=100, learning_rate=0.000001, batch_size=300, reg=1e1)\n",
        "pred = classifier.predict(val_X)\n",
        "accuracy = multiclass_accuracy(pred, val_y)\n",
        "print(\"Accuracy after training for 100 epochs: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FX5pzOZ0BVB",
        "outputId": "b4cfb5fd-4d31-4c51-d7c0-e4d10da90ef9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n",
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n",
            "Epoch 0, loss: nan\n",
            "Epoch 1, loss: nan\n",
            "Epoch 2, loss: nan\n",
            "Epoch 3, loss: nan\n",
            "Epoch 4, loss: nan\n",
            "Epoch 5, loss: nan\n",
            "Epoch 6, loss: nan\n",
            "Epoch 7, loss: nan\n",
            "Epoch 8, loss: nan\n",
            "Epoch 9, loss: nan\n",
            "Epoch 10, loss: nan\n",
            "Epoch 11, loss: nan\n",
            "Epoch 12, loss: nan\n",
            "Epoch 13, loss: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-1ff4a67ab60b>:75: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(mask_target * np.log(probabilities))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14, loss: nan\n",
            "Epoch 15, loss: nan\n",
            "Epoch 16, loss: nan\n",
            "Epoch 17, loss: nan\n",
            "Epoch 18, loss: nan\n",
            "Epoch 19, loss: nan\n",
            "Epoch 20, loss: nan\n",
            "Epoch 21, loss: nan\n",
            "Epoch 22, loss: nan\n",
            "Epoch 23, loss: nan\n",
            "Epoch 24, loss: nan\n",
            "Epoch 25, loss: nan\n",
            "Epoch 26, loss: nan\n",
            "Epoch 27, loss: nan\n",
            "Epoch 28, loss: nan\n",
            "Epoch 29, loss: nan\n",
            "Epoch 30, loss: nan\n",
            "Epoch 31, loss: nan\n",
            "Epoch 32, loss: nan\n",
            "Epoch 33, loss: nan\n",
            "Epoch 34, loss: nan\n",
            "Epoch 35, loss: nan\n",
            "Epoch 36, loss: nan\n",
            "Epoch 37, loss: nan\n",
            "Epoch 38, loss: nan\n",
            "Epoch 39, loss: nan\n",
            "Epoch 40, loss: nan\n",
            "Epoch 41, loss: nan\n",
            "Epoch 42, loss: nan\n",
            "Epoch 43, loss: nan\n",
            "Epoch 44, loss: nan\n",
            "Epoch 45, loss: nan\n",
            "Epoch 46, loss: nan\n",
            "Epoch 47, loss: nan\n",
            "Epoch 48, loss: nan\n",
            "Epoch 49, loss: nan\n",
            "Epoch 50, loss: nan\n",
            "Epoch 51, loss: nan\n",
            "Epoch 52, loss: nan\n",
            "Epoch 53, loss: nan\n",
            "Epoch 54, loss: nan\n",
            "Epoch 55, loss: nan\n",
            "Epoch 56, loss: nan\n",
            "Epoch 57, loss: nan\n",
            "Epoch 58, loss: nan\n",
            "Epoch 59, loss: nan\n",
            "Epoch 60, loss: nan\n",
            "Epoch 61, loss: nan\n",
            "Epoch 62, loss: nan\n",
            "Epoch 63, loss: nan\n",
            "Epoch 64, loss: nan\n",
            "Epoch 65, loss: nan\n",
            "Epoch 66, loss: nan\n",
            "Epoch 67, loss: nan\n",
            "Epoch 68, loss: nan\n",
            "Epoch 69, loss: nan\n",
            "Epoch 70, loss: nan\n",
            "Epoch 71, loss: nan\n",
            "Epoch 72, loss: nan\n",
            "Epoch 73, loss: nan\n",
            "Epoch 74, loss: nan\n",
            "Epoch 75, loss: nan\n",
            "Epoch 76, loss: nan\n",
            "Epoch 77, loss: nan\n",
            "Epoch 78, loss: nan\n",
            "Epoch 79, loss: nan\n",
            "Epoch 80, loss: nan\n",
            "Epoch 81, loss: nan\n",
            "Epoch 82, loss: nan\n",
            "Epoch 83, loss: nan\n",
            "Epoch 84, loss: nan\n",
            "Epoch 85, loss: nan\n",
            "Epoch 86, loss: nan\n",
            "Epoch 87, loss: nan\n",
            "Epoch 88, loss: nan\n",
            "Epoch 89, loss: nan\n",
            "Epoch 90, loss: nan\n",
            "Epoch 91, loss: nan\n",
            "Epoch 92, loss: nan\n",
            "Epoch 93, loss: nan\n",
            "Epoch 94, loss: nan\n",
            "Epoch 95, loss: nan\n",
            "Epoch 96, loss: nan\n",
            "Epoch 97, loss: nan\n",
            "Epoch 98, loss: nan\n",
            "Epoch 99, loss: nan\n",
            "Epoch 100, loss: nan\n",
            "Epoch 101, loss: nan\n",
            "Epoch 102, loss: nan\n",
            "Epoch 103, loss: nan\n",
            "Epoch 104, loss: nan\n",
            "Epoch 105, loss: nan\n",
            "Epoch 106, loss: nan\n",
            "Epoch 107, loss: nan\n",
            "Epoch 108, loss: nan\n",
            "Epoch 109, loss: nan\n",
            "Epoch 110, loss: nan\n",
            "Epoch 111, loss: nan\n",
            "Epoch 112, loss: nan\n",
            "Epoch 113, loss: nan\n",
            "Epoch 114, loss: nan\n",
            "Epoch 115, loss: nan\n",
            "Epoch 116, loss: nan\n",
            "Epoch 117, loss: nan\n",
            "Epoch 118, loss: nan\n",
            "Epoch 119, loss: nan\n",
            "Epoch 120, loss: nan\n",
            "Epoch 121, loss: nan\n",
            "Epoch 122, loss: nan\n",
            "Epoch 123, loss: nan\n",
            "Epoch 124, loss: nan\n",
            "Epoch 125, loss: nan\n",
            "Epoch 126, loss: nan\n",
            "Epoch 127, loss: nan\n",
            "Epoch 128, loss: nan\n",
            "Epoch 129, loss: nan\n",
            "Epoch 130, loss: nan\n",
            "Epoch 131, loss: nan\n",
            "Epoch 132, loss: nan\n",
            "Epoch 133, loss: nan\n",
            "Epoch 134, loss: nan\n",
            "Epoch 135, loss: nan\n",
            "Epoch 136, loss: nan\n",
            "Epoch 137, loss: nan\n",
            "Epoch 138, loss: nan\n",
            "Epoch 139, loss: nan\n",
            "Epoch 140, loss: nan\n",
            "Epoch 141, loss: nan\n",
            "Epoch 142, loss: nan\n",
            "Epoch 143, loss: nan\n",
            "Epoch 144, loss: nan\n",
            "Epoch 145, loss: nan\n",
            "Epoch 146, loss: nan\n",
            "Epoch 147, loss: nan\n",
            "Epoch 148, loss: nan\n",
            "Epoch 149, loss: nan\n",
            "Epoch 150, loss: nan\n",
            "Epoch 151, loss: nan\n",
            "Epoch 152, loss: nan\n",
            "Epoch 153, loss: nan\n",
            "Epoch 154, loss: nan\n",
            "Epoch 155, loss: nan\n",
            "Epoch 156, loss: nan\n",
            "Epoch 157, loss: nan\n",
            "Epoch 158, loss: nan\n",
            "Epoch 159, loss: nan\n",
            "Epoch 160, loss: nan\n",
            "Epoch 161, loss: nan\n",
            "Epoch 162, loss: nan\n",
            "Epoch 163, loss: nan\n",
            "Epoch 164, loss: nan\n",
            "Epoch 165, loss: nan\n",
            "Epoch 166, loss: nan\n",
            "Epoch 167, loss: nan\n",
            "Epoch 168, loss: nan\n",
            "Epoch 169, loss: nan\n",
            "Epoch 170, loss: nan\n",
            "Epoch 171, loss: nan\n",
            "Epoch 172, loss: nan\n",
            "Epoch 173, loss: nan\n",
            "Epoch 174, loss: nan\n",
            "Epoch 175, loss: nan\n",
            "Epoch 176, loss: nan\n",
            "Epoch 177, loss: nan\n",
            "Epoch 178, loss: nan\n",
            "Epoch 179, loss: nan\n",
            "Epoch 180, loss: nan\n",
            "Epoch 181, loss: nan\n",
            "Epoch 182, loss: nan\n",
            "Epoch 183, loss: nan\n",
            "Epoch 184, loss: nan\n",
            "Epoch 185, loss: nan\n",
            "Epoch 186, loss: nan\n",
            "Epoch 187, loss: nan\n",
            "Epoch 188, loss: nan\n",
            "Epoch 189, loss: nan\n",
            "Epoch 190, loss: nan\n",
            "Epoch 191, loss: nan\n",
            "Epoch 192, loss: nan\n",
            "Epoch 193, loss: nan\n",
            "Epoch 194, loss: nan\n",
            "Epoch 195, loss: nan\n",
            "Epoch 196, loss: nan\n",
            "Epoch 197, loss: nan\n",
            "Epoch 198, loss: nan\n",
            "Epoch 199, loss: nan\n",
            "best validation accuracy achieved: 0.055000\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "batch_size = 300\n",
        "\n",
        "learning_rates = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
        "reg_strengths = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
        "\n",
        "best_classifier = ()\n",
        "best_val_accuracy = 0\n",
        "\n",
        "# TODO use validation set to find the best hyperparameters\n",
        "# hint: for best results, you might need to try more values for learning rate and regularization strength\n",
        "# than provided initially\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    for reg in reg_strengths:\n",
        "        classifier.fit(train_X, train_y, epochs=num_epochs,\n",
        "                       learning_rate=learning_rate, batch_size=batch_size, reg=reg)\n",
        "        pred = classifier.predict(val_X)\n",
        "        accuracy = multiclass_accuracy(pred, val_y)\n",
        "        if accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = accuracy\n",
        "            best_classifier = (learning_rate, reg)\n",
        "        print('best validation accuracy achieved: %f' % best_val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJbu15bl1cG0",
        "outputId": "d2ac6a76-27e3-447d-98b8-6c80fe498bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 602.237255\n",
            "Epoch 1, loss: 621.579937\n",
            "Epoch 2, loss: 615.313953\n",
            "Epoch 3, loss: 606.309044\n",
            "Epoch 4, loss: 609.211925\n",
            "Epoch 5, loss: 623.785754\n",
            "Epoch 6, loss: 616.367358\n",
            "Epoch 7, loss: 627.912188\n",
            "Epoch 8, loss: 612.898803\n",
            "Epoch 9, loss: 604.885655\n",
            "Epoch 10, loss: 594.610053\n",
            "Epoch 11, loss: 621.344240\n",
            "Epoch 12, loss: 620.608061\n",
            "Epoch 13, loss: 614.730756\n",
            "Epoch 14, loss: 602.900146\n",
            "Epoch 15, loss: 616.451325\n",
            "Epoch 16, loss: 611.519521\n",
            "Epoch 17, loss: 621.623011\n",
            "Epoch 18, loss: 604.476646\n",
            "Epoch 19, loss: 618.830230\n",
            "Epoch 20, loss: 602.920826\n",
            "Epoch 21, loss: 621.015674\n",
            "Epoch 22, loss: 628.191929\n",
            "Epoch 23, loss: 617.871409\n",
            "Epoch 24, loss: 614.714833\n",
            "Epoch 25, loss: 616.557445\n",
            "Epoch 26, loss: 602.975477\n",
            "Epoch 27, loss: 624.955340\n",
            "Epoch 28, loss: 631.513708\n",
            "Epoch 29, loss: 599.776479\n",
            "Epoch 30, loss: 616.674924\n",
            "Epoch 31, loss: 613.658335\n",
            "Epoch 32, loss: 604.142144\n",
            "Epoch 33, loss: 604.152814\n",
            "Epoch 34, loss: 613.506202\n",
            "Epoch 35, loss: 607.738048\n",
            "Epoch 36, loss: 624.971632\n",
            "Epoch 37, loss: 625.908617\n",
            "Epoch 38, loss: 602.797108\n",
            "Epoch 39, loss: 629.198402\n",
            "Epoch 40, loss: 592.422409\n",
            "Epoch 41, loss: 585.161860\n",
            "Epoch 42, loss: 596.169919\n",
            "Epoch 43, loss: 608.996713\n",
            "Epoch 44, loss: 607.080662\n",
            "Epoch 45, loss: 615.274302\n",
            "Epoch 46, loss: 598.702290\n",
            "Epoch 47, loss: 595.583001\n",
            "Epoch 48, loss: 610.065409\n",
            "Epoch 49, loss: 611.084871\n",
            "Epoch 50, loss: 597.837910\n",
            "Epoch 51, loss: 615.938958\n",
            "Epoch 52, loss: 607.211753\n",
            "Epoch 53, loss: 605.406012\n",
            "Epoch 54, loss: 609.048799\n",
            "Epoch 55, loss: 600.382416\n",
            "Epoch 56, loss: 586.120806\n",
            "Epoch 57, loss: 605.242529\n",
            "Epoch 58, loss: 591.344470\n",
            "Epoch 59, loss: 617.036233\n",
            "Epoch 60, loss: 610.227383\n",
            "Epoch 61, loss: 602.582708\n",
            "Epoch 62, loss: 612.840748\n",
            "Epoch 63, loss: 593.407935\n",
            "Epoch 64, loss: 627.827736\n",
            "Epoch 65, loss: 633.511858\n",
            "Epoch 66, loss: 614.335374\n",
            "Epoch 67, loss: 627.528264\n",
            "Epoch 68, loss: 621.892302\n",
            "Epoch 69, loss: 612.729463\n",
            "Epoch 70, loss: 621.327586\n",
            "Epoch 71, loss: 612.091561\n",
            "Epoch 72, loss: 638.967966\n",
            "Epoch 73, loss: 579.751788\n",
            "Epoch 74, loss: 620.936166\n",
            "Epoch 75, loss: 604.851600\n",
            "Epoch 76, loss: 614.940169\n",
            "Epoch 77, loss: 611.367234\n",
            "Epoch 78, loss: 582.824288\n",
            "Epoch 79, loss: 598.152274\n",
            "Epoch 80, loss: 632.766581\n",
            "Epoch 81, loss: 625.059740\n",
            "Epoch 82, loss: 594.279055\n",
            "Epoch 83, loss: 610.097633\n",
            "Epoch 84, loss: 611.042650\n",
            "Epoch 85, loss: 594.392413\n",
            "Epoch 86, loss: 607.437714\n",
            "Epoch 87, loss: 592.104183\n",
            "Epoch 88, loss: 605.928134\n",
            "Epoch 89, loss: 586.321268\n",
            "Epoch 90, loss: 594.645168\n",
            "Epoch 91, loss: 630.189980\n",
            "Epoch 92, loss: 600.778930\n",
            "Epoch 93, loss: 630.247813\n",
            "Epoch 94, loss: 646.164416\n",
            "Epoch 95, loss: 626.592656\n",
            "Epoch 96, loss: 608.674424\n",
            "Epoch 97, loss: 602.828684\n",
            "Epoch 98, loss: 611.754463\n",
            "Epoch 99, loss: 603.349539\n",
            "Epoch 100, loss: 619.067450\n",
            "Epoch 101, loss: 610.365453\n",
            "Epoch 102, loss: 621.832819\n",
            "Epoch 103, loss: 626.831673\n",
            "Epoch 104, loss: 587.514299\n",
            "Epoch 105, loss: 624.958858\n",
            "Epoch 106, loss: 602.313104\n",
            "Epoch 107, loss: 620.402454\n",
            "Epoch 108, loss: 583.443992\n",
            "Epoch 109, loss: 623.857855\n",
            "Epoch 110, loss: 623.736786\n",
            "Epoch 111, loss: 595.885992\n",
            "Epoch 112, loss: 608.905002\n",
            "Epoch 113, loss: 590.504817\n",
            "Epoch 114, loss: 619.897387\n",
            "Epoch 115, loss: 612.267082\n",
            "Epoch 116, loss: 604.034038\n",
            "Epoch 117, loss: 619.053861\n",
            "Epoch 118, loss: 632.332639\n",
            "Epoch 119, loss: 612.800056\n",
            "Epoch 120, loss: 617.981397\n",
            "Epoch 121, loss: 628.390274\n",
            "Epoch 122, loss: 603.137717\n",
            "Epoch 123, loss: 623.677380\n",
            "Epoch 124, loss: 618.209137\n",
            "Epoch 125, loss: 614.423816\n",
            "Epoch 126, loss: 603.515061\n",
            "Epoch 127, loss: 619.002047\n",
            "Epoch 128, loss: 585.502495\n",
            "Epoch 129, loss: 608.936295\n",
            "Epoch 130, loss: 608.791860\n",
            "Epoch 131, loss: 619.417659\n",
            "Epoch 132, loss: 603.365648\n",
            "Epoch 133, loss: 620.900652\n",
            "Epoch 134, loss: 593.821394\n",
            "Epoch 135, loss: 620.518654\n",
            "Epoch 136, loss: 615.075919\n",
            "Epoch 137, loss: 602.202321\n",
            "Epoch 138, loss: 609.046254\n",
            "Epoch 139, loss: 618.596523\n",
            "Epoch 140, loss: 592.799093\n",
            "Epoch 141, loss: 617.980450\n",
            "Epoch 142, loss: 614.101291\n",
            "Epoch 143, loss: 609.650163\n",
            "Epoch 144, loss: 618.751515\n",
            "Epoch 145, loss: 601.647461\n",
            "Epoch 146, loss: 629.043396\n",
            "Epoch 147, loss: 594.465298\n",
            "Epoch 148, loss: 613.998003\n",
            "Epoch 149, loss: 580.066048\n",
            "Epoch 150, loss: 632.682644\n",
            "Epoch 151, loss: 616.992842\n",
            "Epoch 152, loss: 604.688053\n",
            "Epoch 153, loss: 630.658072\n",
            "Epoch 154, loss: 576.216942\n",
            "Epoch 155, loss: 600.543995\n",
            "Epoch 156, loss: 604.503765\n",
            "Epoch 157, loss: 614.414340\n",
            "Epoch 158, loss: 596.343349\n",
            "Epoch 159, loss: 590.474166\n",
            "Epoch 160, loss: 603.215751\n",
            "Epoch 161, loss: 615.865332\n",
            "Epoch 162, loss: 582.483949\n",
            "Epoch 163, loss: 605.671423\n",
            "Epoch 164, loss: 640.843055\n",
            "Epoch 165, loss: 605.949037\n",
            "Epoch 166, loss: 606.744388\n",
            "Epoch 167, loss: 600.752237\n",
            "Epoch 168, loss: 599.695922\n",
            "Epoch 169, loss: 580.312032\n",
            "Epoch 170, loss: 617.268593\n",
            "Epoch 171, loss: 608.646630\n",
            "Epoch 172, loss: 602.962908\n",
            "Epoch 173, loss: 599.920690\n",
            "Epoch 174, loss: 609.099824\n",
            "Epoch 175, loss: 591.331158\n",
            "Epoch 176, loss: 605.244357\n",
            "Epoch 177, loss: 620.674507\n",
            "Epoch 178, loss: 606.738187\n",
            "Epoch 179, loss: 602.522296\n",
            "Epoch 180, loss: 621.657901\n",
            "Epoch 181, loss: 597.446578\n",
            "Epoch 182, loss: 601.971446\n",
            "Epoch 183, loss: 600.020902\n",
            "Epoch 184, loss: 608.580717\n",
            "Epoch 185, loss: 614.941439\n",
            "Epoch 186, loss: 606.563705\n",
            "Epoch 187, loss: 632.875368\n",
            "Epoch 188, loss: 611.446495\n",
            "Epoch 189, loss: 605.107409\n",
            "Epoch 190, loss: 615.757141\n",
            "Epoch 191, loss: 587.510572\n",
            "Epoch 192, loss: 592.239451\n",
            "Epoch 193, loss: 610.053289\n",
            "Epoch 194, loss: 638.778137\n",
            "Epoch 195, loss: 596.938240\n",
            "Epoch 196, loss: 629.532560\n",
            "Epoch 197, loss: 606.463721\n",
            "Epoch 198, loss: 588.138957\n",
            "Epoch 199, loss: 618.864947\n",
            "Linear softmax classifier test set accuracy: 0.229000\n"
          ]
        }
      ],
      "source": [
        "classifier.fit(train_X, train_y, epochs=num_epochs,\n",
        "                       learning_rate=best_classifier[0], batch_size=batch_size, reg=best_classifier[0])\n",
        "test_pred = classifier.predict(test_X)\n",
        "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
        "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bNa2C5jQGVMkwGl45aelePCk2YCvHmYs",
      "authorship_tag": "ABX9TyPTSDNovHBo8fPG/CrPZKka",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}